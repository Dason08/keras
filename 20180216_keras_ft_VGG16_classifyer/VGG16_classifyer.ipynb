{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今度こそFine-TuningしたVGG16で遊ぼう\n",
    "** 転移学習**とはImageNetのような巨大なデータセットを使って学習した重みを別のニューラルネットワークにコピーして、再学習(**Fine-Tuning**)を行うことである。手元にあるデータセットが少ない場合、特に有効な手法である。今回は、17種類の花の画像分類するツールを作成したいが、データ数が多くないため、VGGと同じ構成のネットワークを用意し、学習済みの重みを初期値として新しいデータセットを対象にfine-tuningするよ。\n",
    "<p>\n",
    "参 考\n",
    "* [転移学習について]https://elix-tech.github.io/ja/2016/06/02/kaggle-facial-keypoints-ja.html\n",
    "* [前半のfine-tuningについて]https://github.com/aidiary/keras-examples/blob/master/vgg16/17flowers/finetuning.py\n",
    "* [後半について]https://github.com/aidiary/keras-examples/blob/master/vgg16/17flowers/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ディレクトリの指定、なければ作成\n",
    "IN_DIR = 'jpg'\n",
    "TRAIN_DIR = 'train_images'\n",
    "TEST_DIR = 'test_images'\n",
    "\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    os.mkdir(TRAIN_DIR)\n",
    "\n",
    "if not os.path.exists(TEST_DIR):\n",
    "    os.mkdir(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.txtのファイルを読み込んで、花の名前を入れるとその花の存在する範囲を返す関数を作るよ\n",
    "# name => (start idx, end idx)\n",
    "flower_dics = {}\n",
    "\n",
    "with open('labels.txt') as fp:\n",
    "    for line in fp:\n",
    "        line = line.rstrip() #文字列を右から削除\n",
    "        cols = line.split() #文字列を分割\n",
    "\n",
    "        assert len(cols) == 3 #超お手軽にテストする。条件式がFalseならアラート\n",
    "\n",
    "        start = int(cols[0])\n",
    "        end = int(cols[1])\n",
    "        name = cols[2]\n",
    "\n",
    "        flower_dics[name] = (start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 花ごとのディレクトリを作成\n",
    "for name in flower_dics:\n",
    "    os.mkdir(os.path.join(TRAIN_DIR, name))\n",
    "    os.mkdir(os.path.join(TEST_DIR, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpgをスキャン\n",
    "for f in sorted(os.listdir(IN_DIR)):\n",
    "    # image_0001.jpg => 1\n",
    "    prefix = f.replace('.jpg', '') #.jpg を (空白) に置き換える\n",
    "    idx = int(prefix.split('_',1)[1]) #'_'の前と後に分ける。その右の方を選び、それが数字だからint型にする。\n",
    "\n",
    "    for name in flower_dics:\n",
    "        start, end = flower_dics[name]\n",
    "        if idx in range(start, end + 1):\n",
    "            source = os.path.join(IN_DIR, f) \n",
    "            dest = os.path.join(TRAIN_DIR, name)\n",
    "            shutil.copy(source, dest)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの各ディレクトリからランダムに10枚をテストとする\n",
    "for d in os.listdir(TRAIN_DIR):\n",
    "    files = os.listdir(os.path.join(TRAIN_DIR, d)) #pathをtrain_images/[花]として１つ１つ繋げる\n",
    "    random.shuffle(files) #花のファイル内の画像をシャッフル\n",
    "    for f in files[:10]: #シャッフルしたファイルから10個選び、それをfに１つ１つ入れていく。\n",
    "        source = os.path.join(TRAIN_DIR, d, f)  #train_image/[花]/[画像]のpathをつなぐ\n",
    "        dest = os.path.join(TEST_DIR, d) #test_image/[花]をつなぐ\n",
    "        shutil.move(source, dest) #sourceをdestファイルに移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果を格納するディレクトリ作成\n",
    "result_dir = 'results'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "    \n",
    "#結果を保存する関数の定義\n",
    "def save_history(history, result_file):\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = len(acc)\n",
    "\n",
    "    with open(result_file, \"w\") as fp:\n",
    "        fp.write(\"epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n\")\n",
    "        for i in range(epochs):\n",
    "            fp.write(\"%d\\t%f\\t%f\\t%f\\t%f\\n\" % (i, loss[i], acc[i], val_loss[i], val_acc[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Tulip', 'Snowdrop', 'LilyValley', 'Bluebell', 'Crocus',\n",
    "           'Iris', 'Tigerlily', 'Daffodil', 'Fritillary', 'Sunflower',\n",
    "           'Daisy', 'ColtsFoot', 'Dandelion', 'Cowslip', 'Buttercup',\n",
    "           'Windflower', 'Pansy']\n",
    "batch_size = 32\n",
    "nb_classes = len(classes)\n",
    "img_rows, img_cols = 150, 150\n",
    "channels = 3\n",
    "epochs = 50\n",
    "train_samples = 1190\n",
    "val_samples = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16モデルと学習済み重みをロード\n",
    "# 全結合層（FC）はいらないのでinclude_top=False\n",
    "input_tensor = Input(shape=(img_rows, img_cols, 3))\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "# FC層を構築\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# VGG16とFCを接続\n",
    "model = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後のconv層の直前までの層をfreeze\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Fine-tuningのときはSGDの方がよい？\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1120 images belonging to 17 classes.\n",
      "Found 160 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    color_mode='rgb',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    color_mode='rgb',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 135s 4s/step - loss: 3.0267 - acc: 0.0723 - val_loss: 2.6238 - val_acc: 0.1750\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 130s 4s/step - loss: 2.6152 - acc: 0.1652 - val_loss: 2.3623 - val_acc: 0.3187\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 2.3438 - acc: 0.2598 - val_loss: 2.0880 - val_acc: 0.4000\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 132s 4s/step - loss: 2.1037 - acc: 0.3634 - val_loss: 1.8052 - val_acc: 0.5437\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.8611 - acc: 0.4366 - val_loss: 1.5578 - val_acc: 0.5938\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.6617 - acc: 0.4893 - val_loss: 1.3477 - val_acc: 0.6500\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.4831 - acc: 0.5527 - val_loss: 1.1804 - val_acc: 0.6813\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.2992 - acc: 0.6214 - val_loss: 1.0483 - val_acc: 0.7312\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.1421 - acc: 0.6500 - val_loss: 0.9358 - val_acc: 0.7312\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 1.0727 - acc: 0.6696 - val_loss: 0.8382 - val_acc: 0.7375\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.9636 - acc: 0.7098 - val_loss: 0.7939 - val_acc: 0.7375\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.8193 - acc: 0.7554 - val_loss: 0.6979 - val_acc: 0.7937\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.7457 - acc: 0.7929 - val_loss: 0.6505 - val_acc: 0.8063\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.7523 - acc: 0.7643 - val_loss: 0.6629 - val_acc: 0.8063\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.6508 - acc: 0.8152 - val_loss: 0.5862 - val_acc: 0.8250\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.6002 - acc: 0.8125 - val_loss: 0.5948 - val_acc: 0.8250\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.5893 - acc: 0.8366 - val_loss: 0.5382 - val_acc: 0.8313\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.5488 - acc: 0.8241 - val_loss: 0.5195 - val_acc: 0.8187\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.5531 - acc: 0.8330 - val_loss: 0.5226 - val_acc: 0.8250\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.5181 - acc: 0.8393 - val_loss: 0.5005 - val_acc: 0.8125\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 133s 4s/step - loss: 0.4464 - acc: 0.8723 - val_loss: 0.4489 - val_acc: 0.8375\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 0.4221 - acc: 0.8687 - val_loss: 0.4218 - val_acc: 0.8625\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 0.3837 - acc: 0.8911 - val_loss: 0.4009 - val_acc: 0.8750\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 0.3724 - acc: 0.8893 - val_loss: 0.4209 - val_acc: 0.8812\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 0.3631 - acc: 0.8830 - val_loss: 0.3910 - val_acc: 0.8875\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 134s 4s/step - loss: 0.3559 - acc: 0.8911 - val_loss: 0.4268 - val_acc: 0.8500\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.3146 - acc: 0.9036 - val_loss: 0.3648 - val_acc: 0.8562\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.3121 - acc: 0.9071 - val_loss: 0.3553 - val_acc: 0.8750\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.3100 - acc: 0.9009 - val_loss: 0.3569 - val_acc: 0.8812\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.3116 - acc: 0.9107 - val_loss: 0.3397 - val_acc: 0.8812\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2652 - acc: 0.9179 - val_loss: 0.3369 - val_acc: 0.8938\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2400 - acc: 0.9286 - val_loss: 0.3146 - val_acc: 0.8938\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2199 - acc: 0.9357 - val_loss: 0.3224 - val_acc: 0.8938\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2355 - acc: 0.9232 - val_loss: 0.2993 - val_acc: 0.9062\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2178 - acc: 0.9339 - val_loss: 0.2730 - val_acc: 0.9313\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2312 - acc: 0.9286 - val_loss: 0.3004 - val_acc: 0.9125\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.2157 - acc: 0.9348 - val_loss: 0.2792 - val_acc: 0.9062\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1802 - acc: 0.9500 - val_loss: 0.2794 - val_acc: 0.9125\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1796 - acc: 0.9527 - val_loss: 0.2983 - val_acc: 0.9187\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1776 - acc: 0.9438 - val_loss: 0.2879 - val_acc: 0.9062\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1842 - acc: 0.9500 - val_loss: 0.3029 - val_acc: 0.9250\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1649 - acc: 0.9455 - val_loss: 0.2736 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1508 - acc: 0.9607 - val_loss: 0.2895 - val_acc: 0.9062\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1670 - acc: 0.9473 - val_loss: 0.2712 - val_acc: 0.9125\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1517 - acc: 0.9625 - val_loss: 0.2488 - val_acc: 0.9187\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1461 - acc: 0.9607 - val_loss: 0.2741 - val_acc: 0.9187\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1306 - acc: 0.9625 - val_loss: 0.2719 - val_acc: 0.9062\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1266 - acc: 0.9661 - val_loss: 0.2463 - val_acc: 0.9313\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1095 - acc: 0.9741 - val_loss: 0.2429 - val_acc: 0.9313\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 131s 4s/step - loss: 0.1191 - acc: 0.9723 - val_loss: 0.2640 - val_acc: 0.9187\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(result_dir, 'finetuning.h5'))\n",
    "save_history(history, os.path.join(result_dir, 'history_finetuning.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(result_dir, 'finetuning.h5'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input,decode_predictions\n",
    "\n",
    "img_path = 'HIMAWARI.jpg'\n",
    "img = image.load_img(img_path, target_size=(img_rows, img_cols))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスを予測\n",
    "# 入力は1枚の画像なので[0]のみ\n",
    "pred = model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sunflower', 0.99974376)\n",
      "('Daisy', 0.00024026785)\n",
      "('Tulip', 8.3102677e-06)\n",
      "('Pansy', 3.9450715e-06)\n",
      "('ColtsFoot', 2.0047867e-06)\n"
     ]
    }
   ],
   "source": [
    "# 予測確率が高いトップ5を出力\n",
    "top = 5\n",
    "top_indices = pred.argsort()[-top:][::-1]\n",
    "result = [(classes[i], pred[i]) for i in top_indices]\n",
    "for x in result:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
